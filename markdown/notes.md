# Notes

-   bn backprop: https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html
-   bn backprop: http://cthorey.github.io./backpropagation/
-   autograd.function: https://pytorch.org/docs/stable/notes/extending.html
-   layernorm: https://qywu.github.io/2019/05/22/explore-gradient-checkpointing.html
-   inplace abn: https://github.com/mapillary/inplace_abn
-   bn mean over axis: https://forums.fast.ai/t/batchnormalization-axis-1-when-used-on-convolutional-layers/214/12

# Todo

-   combine to make activated batchnorm
-   make checkpointing version
-   make inplace abn

-   invertible autograd.function GeLU
-   inplace layer norm

# Done

-   make custom bn and compare speed
-   check memory usage
-   make `autograd.function` relu
